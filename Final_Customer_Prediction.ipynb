{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajinkyajumde/customer-prediction/blob/main/Final_Customer_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tzNkH4pPTGxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90a001a-52c2-4e07-c943-89ea79c76f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "#importing all the neccessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import ensemble\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "from imblearn.over_sampling import ADASYN\n",
        "# Importing XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "# Impoting metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "# Importing scikit logistic regression module\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Impoting metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "# Importing libraries for cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Importing random forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1foyqW2GTYaR"
      },
      "outputs": [],
      "source": [
        "#mounting the google drive for importing the csv files uploaded on the drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4flj--6iCAb"
      },
      "outputs": [],
      "source": [
        "#assigning the neccessary path to create a dataframe for the pandas operation\n",
        "data='/content/drive/MyDrive/Almabetter projects/Customer prediction/train_wn75k28.csv'\n",
        "test_dataset=pd.read_csv(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCYAemvOGAmJ"
      },
      "source": [
        "## **DATA INSPECTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98BpkLUhiOKt"
      },
      "outputs": [],
      "source": [
        "test_dataset.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m8T30tniRJ6"
      },
      "outputs": [],
      "source": [
        "test_dataset.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW2UVf9qic5R"
      },
      "outputs": [],
      "source": [
        "test_dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhj45tEXil-6"
      },
      "outputs": [],
      "source": [
        "test_dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0thTn0fjBYA"
      },
      "outputs": [],
      "source": [
        "test_dataset=test_dataset.drop(columns=['signup_date','id'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_JmBBp9iu51"
      },
      "outputs": [],
      "source": [
        "test_dataset=test_dataset.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNtvebjGi33V"
      },
      "outputs": [],
      "source": [
        "test_dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-D4nBnsjT1E"
      },
      "outputs": [],
      "source": [
        "test_dataset['buy'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLAxZcX7gsgX"
      },
      "outputs": [],
      "source": [
        "for i in list(test_dataset.describe()):\n",
        "  print(f'The Value count for {i} is','\\n')\n",
        "  print(test_dataset[i].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ1czxeJijOk"
      },
      "outputs": [],
      "source": [
        "test_dataset=test_dataset.drop(columns=['user_activity_var_2','user_activity_var_3','user_activity_var_4','user_activity_var_9','user_activity_var_10','user_activity_var_12'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNDcdaE9GHwd"
      },
      "source": [
        "## **EXPLORATORY DATA ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmV9SbBmRPlj"
      },
      "outputs": [],
      "source": [
        "buyers=test_dataset[test_dataset['buy']==1]\n",
        "non_buyers=test_dataset[test_dataset['buy']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIEaDB9RP77q"
      },
      "outputs": [],
      "source": [
        "# Distribution plot\n",
        "for i in list(test_dataset.describe()):\n",
        "   plt.figure(figsize=(8,5))\n",
        "   ax = sns.distplot(buyers[i],label='buy',hist=False)\n",
        "   ax = sns.distplot(non_buyers[i],label='didnt buy',hist=False)\n",
        "   ax.set(xlabel=i)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POt-srnU5roM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK_xSlu-5zjE"
      },
      "outputs": [],
      "source": [
        "calc_vif(test_dataset[[i for i in test_dataset.describe().columns if i not in ['buy']]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du1FKdLkn98j"
      },
      "source": [
        "campaign_var_2 is the feature having VIF greater than 5 so we drop it in order to reduce the multi collinearity in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIWLtzdu6Dli"
      },
      "outputs": [],
      "source": [
        "test_dataset=test_dataset.drop(columns=['campaign_var_2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH57b4bA6KqH"
      },
      "outputs": [],
      "source": [
        "calc_vif(test_dataset[[i for i in test_dataset.describe().columns if i not in ['buy']]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPtqPIp2oJWX"
      },
      "source": [
        "Here we conclude that the dataset is free from multi collinearity and we can proceed further with the modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL9OaFP29U5t"
      },
      "outputs": [],
      "source": [
        "# splitting \"Date\" column into three other columns like \"year\",\"month\",\"day\".\n",
        "test_dataset['created_at'] = test_dataset['created_at'].apply(lambda x:dt.datetime.strptime(x,\"%Y-%m-%d\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2yR_Ig9DOA3"
      },
      "outputs": [],
      "source": [
        "test_dataset['year'] = test_dataset['created_at'].dt.year\n",
        "test_dataset['month'] = test_dataset['created_at'].dt.month\n",
        "test_dataset['day'] = test_dataset['created_at'].dt.day_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGsZARBdDO3l"
      },
      "outputs": [],
      "source": [
        "test_dataset=test_dataset.drop(columns=['created_at','year'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNyCVA-EoZbR"
      },
      "outputs": [],
      "source": [
        "buyers=test_dataset[test_dataset['buy']==1]\n",
        "non_buyers=test_dataset[test_dataset['buy']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSNexhNjoa0n"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "ax = sns.distplot(buyers['month'],label='buy',hist=False)\n",
        "ax = sns.distplot(non_buyers['month'],label='didnt buy',hist=False)\n",
        "ax.set(xlabel=i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8yRZHn2Cj5t"
      },
      "outputs": [],
      "source": [
        "lst=list(test_dataset.columns)\n",
        "lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whcV-Si79GoF"
      },
      "outputs": [],
      "source": [
        "test_dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJHKIHe-ZQCK"
      },
      "outputs": [],
      "source": [
        "lst=lst.remove('buy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzeCWs71pYqM"
      },
      "source": [
        "Here we finalise the list of columns(independent variables) to work with in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fff8WvqHY5Lb"
      },
      "outputs": [],
      "source": [
        "test_dataset=pd.get_dummies(test_dataset, columns=['month','day'], prefix=['month','day'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMR3Cq-uve9K"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn0Nk3Dvj9eZ"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=test_dataset['buy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2MYxbD6pxf-"
      },
      "source": [
        "Based on the above visualisation it is clear that the given dataset in highly imbalanced in terms of dependent variable. So we proceed with the oversampling technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6I1u91GoTFT"
      },
      "outputs": [],
      "source": [
        "#  # Data for all the independent variables\n",
        "X = test_dataset.drop(labels='buy',axis=1)\n",
        "\n",
        "#  # Data for the dependent variable\n",
        "Y = test_dataset['buy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvV1PIwju8fa"
      },
      "outputs": [],
      "source": [
        "test_columns=list(X.columns)\n",
        "test_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG5-y-ZwTF4w"
      },
      "outputs": [],
      "source": [
        "X.shape, Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM_hfOt--ri9"
      },
      "outputs": [],
      "source": [
        "X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeFm29kJ_yoQ"
      },
      "outputs": [],
      "source": [
        "l=['campaign_var_1', 'products_purchased', 'user_activity_var_1',\n",
        "       'user_activity_var_5', 'user_activity_var_6', 'user_activity_var_7',\n",
        "       'user_activity_var_8', 'user_activity_var_11', 'month_1', 'month_2',\n",
        "       'month_3', 'day_Friday',\n",
        "       'day_Monday', 'day_Saturday', 'day_Sunday', 'day_Thursday',\n",
        "       'day_Tuesday', 'day_Wednesday']\n",
        "X=X[l]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=X.fillna(0)"
      ],
      "metadata": {
        "id": "2r7CqqN-WqT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMPrQ-6rTF15"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oJ7lfCHqGnS"
      },
      "source": [
        "AdaSyn (Adaptive Synthetic Sampling):-\n",
        "We use this in order to avoid the dominance of high imbalance of the dataset on our model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wonoyMYdjGVB"
      },
      "outputs": [],
      "source": [
        "ada = ADASYN()\n",
        "X_resampled, y_resampled = ada.fit_resample(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpl0RbjtjGVS"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train=X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnCwRVwNqX7e"
      },
      "source": [
        "Here we get resampled data out of X train and Y train which is balance and so we will be able to train our model based on this sampled dataset to get highly accurate model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw15PWlG44CO"
      },
      "outputs": [],
      "source": [
        "Y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiF-sNv4TFy1"
      },
      "outputs": [],
      "source": [
        "# Check the shape of train dataset\n",
        "print(X_train.shape,Y_train.shape)\n",
        "\n",
        "# Check the shape of test dataset\n",
        "print(X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BqyIQy1R-_M"
      },
      "source": [
        "## **LOGISTIC REGRESSION MODEL WITH K FOLD CROSS VALIDATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv8ROEjd_BiS"
      },
      "outputs": [],
      "source": [
        "# Creating KFold object with 5 splits\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=4)\n",
        "\n",
        "# Specify params\n",
        "params = {\"C\": [0.001,0.05,0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Specifing score as recall as we are more focused on acheiving the higher sensitivity than the accuracy\n",
        "model_cv = GridSearchCV(estimator = LogisticRegression(),\n",
        "                        param_grid = params, \n",
        "                        scoring= 'roc_auc', \n",
        "                        cv = folds, \n",
        "                        verbose = 1,\n",
        "                        return_train_score=True) \n",
        "\n",
        "# Fit the model\n",
        "model_cv.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTTpWfUL_BeO"
      },
      "outputs": [],
      "source": [
        "# results of grid search CV\n",
        "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
        "cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZxAWoHk_BaO"
      },
      "outputs": [],
      "source": [
        "# plot of C versus train and validation scores\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(cv_results['param_C'], cv_results['mean_test_score'])\n",
        "plt.plot(cv_results['param_C'], cv_results['mean_train_score'])\n",
        "plt.xlabel('C')\n",
        "plt.ylabel('roc_auc')\n",
        "plt.legend(['test result', 'train result'], loc='upper left')\n",
        "plt.xscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OD_z1om_BWf"
      },
      "outputs": [],
      "source": [
        "# Best score with best C\n",
        "best_score = model_cv.best_score_\n",
        "best_C = model_cv.best_params_['C']\n",
        "\n",
        "print(\" The highest test roc_auc is {0} at C = {1}\".format(best_score, best_C))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROwBTfEX_BSq"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model with best C\n",
        "logistic_imb = LogisticRegression(C=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgl6ZIrf_11Y"
      },
      "outputs": [],
      "source": [
        "# Fit the model on the train set\n",
        "logistic_imb_model = logistic_imb.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFYalbs7_8Ct"
      },
      "outputs": [],
      "source": [
        "# Predictions on the train set\n",
        "Y_train_pred = logistic_imb_model.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdK0EWXk__Ft"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "confusion = metrics.confusion_matrix(Y_train, Y_train_pred)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EbwqVsMAMPA"
      },
      "outputs": [],
      "source": [
        "TP = confusion[1,1] # true positive \n",
        "TN = confusion[0,0] # true negatives\n",
        "FP = confusion[0,1] # false positives\n",
        "FN = confusion[1,0] # false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyEKkIBdAR10"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "print(\"Accuracy:-\",metrics.accuracy_score(Y_train, Y_train_pred))\n",
        "\n",
        "# Sensitivity\n",
        "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
        "\n",
        "# Specificity\n",
        "print(\"Specificity:-\", TN / float(TN+FP))\n",
        "\n",
        "# F1 score\n",
        "print(\"F1-Score:-\", f1_score(Y_train, Y_train_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGYkW1IuRzzn"
      },
      "outputs": [],
      "source": [
        "# Predictions on the train set\n",
        "Y_test_pred = logistic_imb_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3we6vPraRzzo"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "confusion = metrics.confusion_matrix(Y_test, Y_test_pred)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIdBwbTLRzzo"
      },
      "outputs": [],
      "source": [
        "TP = confusion[1,1] # true positive \n",
        "TN = confusion[0,0] # true negatives\n",
        "FP = confusion[0,1] # false positives\n",
        "FN = confusion[1,0] # false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UecCFnLtRzzo"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "print(\"Accuracy:-\",metrics.accuracy_score(Y_test, Y_test_pred))\n",
        "\n",
        "# Sensitivity\n",
        "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
        "\n",
        "# Specificity\n",
        "print(\"Specificity:-\", TN / float(TN+FP))\n",
        "\n",
        "# F1 score\n",
        "print(\"F1-Score:-\", f1_score(Y_test, Y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg4ekAytHGsC"
      },
      "source": [
        "## **RANDOM FOREST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvfbObHUHK3v"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'max_depth': range(5,10,5),\n",
        "    'min_samples_leaf': range(50, 150, 50),\n",
        "    'min_samples_split': range(50, 150, 50),\n",
        "    'n_estimators': [100,200,300], \n",
        "    \n",
        "}\n",
        "# Create a based model\n",
        "rf = RandomForestClassifier()\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, \n",
        "                           param_grid = param_grid, \n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt2ZoBsdHK0X"
      },
      "outputs": [],
      "source": [
        "# printing the optimal accuracy score and hyperparameters\n",
        "print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AL3wZ0NHKw1"
      },
      "outputs": [],
      "source": [
        "# model with the best hyperparameters\n",
        "\n",
        "rfc_imb_model = RandomForestClassifier(bootstrap=True,\n",
        "                             max_depth=5,\n",
        "                             min_samples_leaf=50, \n",
        "                             min_samples_split=50,\n",
        "                             \n",
        "                             n_estimators=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-sBiQGtHKtT"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "rfc_imb_model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaty0AEAHKp0"
      },
      "outputs": [],
      "source": [
        "# Predictions on the train set\n",
        "Y_train_pred = rfc_imb_model.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWCdXSLfHaPM"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "confusion = metrics.confusion_matrix(Y_train, Y_train_pred)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udCx9azqHaLm"
      },
      "outputs": [],
      "source": [
        "TP = confusion[1,1] # true positive \n",
        "TN = confusion[0,0] # true negatives\n",
        "FP = confusion[0,1] # false positives\n",
        "FN = confusion[1,0] # false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1KPQhuJHaIc"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "print(\"Accuracy:-\",metrics.accuracy_score(Y_train, Y_train_pred))\n",
        "\n",
        "# Sensitivity\n",
        "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
        "\n",
        "# Specificity\n",
        "print(\"Specificity:-\", TN / float(TN+FP))\n",
        "\n",
        "# F1 score\n",
        "print(\"F1-Score:-\", f1_score(Y_train, Y_train_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFvJC_M3Mehs"
      },
      "outputs": [],
      "source": [
        "# Predictions on the train set\n",
        "Y_test_pred = rfc_imb_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw9By-wCMeht"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "confusion = metrics.confusion_matrix(Y_test, Y_test_pred)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgSKtmq_Meht"
      },
      "outputs": [],
      "source": [
        "TP = confusion[1,1] # true positive \n",
        "TN = confusion[0,0] # true negatives\n",
        "FP = confusion[0,1] # false positives\n",
        "FN = confusion[1,0] # false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc6nz0IfMeht"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "print(\"Accuracy:-\",metrics.accuracy_score(Y_test, Y_test_pred))\n",
        "\n",
        "# Sensitivity\n",
        "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
        "\n",
        "# Specificity\n",
        "print(\"Specificity:-\", TN / float(TN+FP))\n",
        "\n",
        "# F1 score\n",
        "print(\"F1-Score:-\", f1_score(Y_test, Y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsBjMrtYRvzi"
      },
      "source": [
        "## **KNN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owhe4bwPOmWl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#Setup arrays to store training and test accuracies\n",
        "neighbors = np.array([2,5,10,20])\n",
        "train_accuracy =np.empty(len(neighbors))\n",
        "test_accuracy = np.empty(len(neighbors))\n",
        "\n",
        "for i,k in enumerate(neighbors):\n",
        "    # Setup a knn classifier with k neighbors\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    \n",
        "    # Fit the model\n",
        "    knn.fit(X_train, Y_train)\n",
        "    \n",
        "    # Compute accuracy on the training set\n",
        "    train_accuracy[i] = knn.score(X_train, Y_train)\n",
        "    \n",
        "    # Compute accuracy on the test set\n",
        "    test_accuracy[i] = knn.score(X_test, Y_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qh6tl-yO_Pj"
      },
      "outputs": [],
      "source": [
        "# Generate plot\n",
        "plt.title('k-NN Varying number of neighbors')\n",
        "plt.plot(neighbors, test_accuracy, label='Testing Accuracy')\n",
        "plt.plot(neighbors, train_accuracy, label='Training accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of neighbors')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3Md8rGhSFqZ"
      },
      "outputs": [],
      "source": [
        "# Setup a knn classifier with k neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVVvYInXSORv"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "kn_model=knn.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuQrGucKPVwx"
      },
      "outputs": [],
      "source": [
        "# Predictions on the train set\n",
        "Y_train_pred = kn_model.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUJEx47ePVwy"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "confusion = metrics.confusion_matrix(Y_train, Y_train_pred)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZlVJ_t6PVwy"
      },
      "outputs": [],
      "source": [
        "TP = confusion[1,1] # true positive \n",
        "TN = confusion[0,0] # true negatives\n",
        "FP = confusion[0,1] # false positives\n",
        "FN = confusion[1,0] # false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmfq7lrUPVwy"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "print(\"Accuracy:-\",metrics.accuracy_score(Y_train, Y_train_pred))\n",
        "\n",
        "# Sensitivity\n",
        "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
        "\n",
        "# Specificity\n",
        "print(\"Specificity:-\", TN / float(TN+FP))\n",
        "\n",
        "# F1 score\n",
        "print(\"F1-Score:-\", f1_score(Y_train, Y_train_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxMTe7c_RTfS"
      },
      "outputs": [],
      "source": [
        "# Predictions on the train set\n",
        "Y_test_pred = kn_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHWMEMMxRJ9O"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "confusion = metrics.confusion_matrix(Y_test, Y_test_pred)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqAGQ2ElRJ9O"
      },
      "outputs": [],
      "source": [
        "TP = confusion[1,1] # true positive \n",
        "TN = confusion[0,0] # true negatives\n",
        "FP = confusion[0,1] # false positives\n",
        "FN = confusion[1,0] # false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh96ApDjRJ9P"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "print(\"Accuracy:-\",metrics.accuracy_score(Y_test, Y_test_pred))\n",
        "\n",
        "# Sensitivity\n",
        "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
        "\n",
        "# Specificity\n",
        "print(\"Specificity:-\", TN / float(TN+FP))\n",
        "\n",
        "# F1 score\n",
        "print(\"F1-Score:-\", f1_score(Y_test, Y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wElIX3msFtd2"
      },
      "source": [
        "## **XGBOOST ALGORITHM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcblD3z-MNEc"
      },
      "outputs": [],
      "source": [
        "# hyperparameter tuning with XGBoost\n",
        "\n",
        "# creating a KFold object \n",
        "folds = 3\n",
        "\n",
        "# specify range of hyperparameters\n",
        "param_grid = {'learning_rate': [0.01,0.1,0.5,1,10,50], \n",
        "             'subsample': [0.3, 0.6, 0.9,1]}          \n",
        "\n",
        "\n",
        "# specify model\n",
        "xgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n",
        "\n",
        "# set up GridSearchCV()\n",
        "model_cv = GridSearchCV(estimator = xgb_model, \n",
        "                        param_grid = param_grid, \n",
        "                        scoring= 'roc_auc', \n",
        "                        cv = folds, \n",
        "                        verbose = 1,\n",
        "                        return_train_score=True)      \n",
        "\n",
        "# fit the model\n",
        "model_cv.fit(X_train, Y_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwOGpzvrNhfb"
      },
      "outputs": [],
      "source": [
        "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
        "cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8INRs-lQOGTm"
      },
      "outputs": [],
      "source": [
        "# # plotting\n",
        "plt.figure(figsize=(16,6))\n",
        "\n",
        "param_grid = {'learning_rate': [0.01,0.1,0.5,1,10], \n",
        "             'subsample': [0.3, 0.6, 0.9,1]} \n",
        "\n",
        "\n",
        "for n, subsample in enumerate(param_grid['subsample']):\n",
        "    \n",
        "\n",
        "    # subplot 1/n\n",
        "    plt.subplot(1,len(param_grid['subsample']), n+1)\n",
        "    df = cv_results[cv_results['param_subsample']==subsample]\n",
        "\n",
        "    plt.plot(df[\"param_learning_rate\"], df[\"mean_test_score\"])\n",
        "    plt.plot(df[\"param_learning_rate\"], df[\"mean_train_score\"])\n",
        "    plt.xlabel('learning_rate')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.title(\"subsample={0}\".format(subsample))\n",
        "    plt.ylim([0.60, 1])\n",
        "    plt.legend(['test score', 'train score'], loc='upper left')\n",
        "    plt.xscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGJYXX2XOlMs"
      },
      "outputs": [],
      "source": [
        "model_cv.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZkz0XY1Oqft"
      },
      "outputs": [],
      "source": [
        "# chosen hyperparameters\n",
        "# 'objective':'binary:logistic' outputs probability rather than label, which we need for calculating auc\n",
        "params = {'learning_rate': 1,\n",
        "           \n",
        "          \n",
        "          'subsample':0.6,\n",
        "         'objective':'binary:logistic'}\n",
        "\n",
        "# fit model on training data\n",
        "xgb_imb_model = XGBClassifier(params = params)\n",
        "xgb_imb_model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGZxkZf6Oxf0"
      },
      "outputs": [],
      "source": [
        "# Predictions on the train set\n",
        "Y_train_pred = xgb_imb_model.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpDoQhz2O6Oe"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "confusion = confusion_matrix(Y_train, Y_train_pred)\n",
        "print(confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QktQZfIMJdka"
      },
      "outputs": [],
      "source": [
        "TP = confusion[1,1] # true positive \n",
        "TN = confusion[0,0] # true negatives\n",
        "FP = confusion[0,1] # false positives\n",
        "FN = confusion[1,0] # false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKVyYQyYJdkb"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "print(\"Accuracy:-\",metrics.accuracy_score(Y_train, Y_train_pred))\n",
        "\n",
        "# Sensitivity\n",
        "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
        "\n",
        "# Specificity\n",
        "print(\"Specificity:-\", TN / float(TN+FP))\n",
        "\n",
        "# F1 score\n",
        "print(\"F1-Score:-\", f1_score(Y_train, Y_train_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh-yWdMs85CF"
      },
      "outputs": [],
      "source": [
        "Y_test_pred = xgb_imb_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABX-sbTF84-p"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "confusion = confusion_matrix(Y_test, Y_test_pred)\n",
        "print(confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfAwA7pvJi-F"
      },
      "outputs": [],
      "source": [
        "TP = confusion[1,1] # true positive \n",
        "TN = confusion[0,0] # true negatives\n",
        "FP = confusion[0,1] # false positives\n",
        "FN = confusion[1,0] # false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJXee56gJi-F"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "print(\"Accuracy:-\",metrics.accuracy_score(Y_train, Y_train_pred))\n",
        "\n",
        "# Sensitivity\n",
        "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
        "\n",
        "# Specificity\n",
        "print(\"Specificity:-\", TN / float(TN+FP))\n",
        "\n",
        "# F1 score\n",
        "print(\"F1-Score:-\", f1_score(Y_test, Y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmpOTf2gthk-"
      },
      "source": [
        "## **PREDICTING ON THE GIVEN DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id-eDihvtut9"
      },
      "source": [
        "We use XGB Model for predicting on the given dataset as it has higher accuracy as compared to other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkN4XLsht4EU"
      },
      "outputs": [],
      "source": [
        "data='/content/drive/MyDrive/Almabetter projects/Customer prediction/test_Wf7sxXF.csv'\n",
        "test_dataset=pd.read_csv(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-LOf2Br9Aj6"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaotOAfX9GsQ"
      },
      "outputs": [],
      "source": [
        "index=test_dataset['id'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61P3RiGJ9Rvu"
      },
      "outputs": [],
      "source": [
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1GzeSZV9VSK"
      },
      "outputs": [],
      "source": [
        "\n",
        "lst=['campaign_var_1',\n",
        " 'products_purchased',\n",
        " 'user_activity_var_1',\n",
        " 'user_activity_var_5',\n",
        " 'user_activity_var_6',\n",
        " 'user_activity_var_7',\n",
        " 'user_activity_var_8',\n",
        " 'user_activity_var_11','month','day']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27EjebQD99bU"
      },
      "outputs": [],
      "source": [
        "# splitting \"Date\" column into three other columns like \"year\",\"month\",\"day\".\n",
        "test_dataset['created_at'] = test_dataset['created_at'].apply(lambda x:dt.datetime.strptime(x,\"%Y-%m-%d\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L55SlFA99bk"
      },
      "outputs": [],
      "source": [
        "test_dataset['year'] = test_dataset['created_at'].dt.year\n",
        "test_dataset['month'] = test_dataset['created_at'].dt.month\n",
        "test_dataset['day'] = test_dataset['created_at'].dt.day_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XEUEDTouz0P"
      },
      "outputs": [],
      "source": [
        "test_dataset=test_dataset[lst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C1g_o7445Bh"
      },
      "outputs": [],
      "source": [
        "test_dataset=pd.get_dummies(test_dataset, columns=['month','day'], prefix=['month','day'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDOvt0Qv-E4e"
      },
      "outputs": [],
      "source": [
        "test_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URxypE2V-a5C"
      },
      "outputs": [],
      "source": [
        "test_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGsbt62o_U_r"
      },
      "outputs": [],
      "source": [
        "lst=list(test_dataset.columns)\n",
        "lst"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset=test_dataset.fillna(0)"
      ],
      "metadata": {
        "id": "dagun8ZLW75u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRWUvOTt5fev"
      },
      "outputs": [],
      "source": [
        "# Predictions on the train set\n",
        "Y_train_pred = xgb_imb_model.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cb9fsfGVW3VG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final Customer Prediction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOtFBzywq2TPPo98Ies1wtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}